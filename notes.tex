\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry} % lots more margin
\pagenumbering{gobble} % ignore page numbers

\usepackage{titling}
\setlength{\droptitle}{-0.75in}

\setlength{\parindent}{0cm}

\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref} % for nice looking urls
\usepackage{booktabs} % for making tables
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{multicol}

\usepackage{titlesec}
\usepackage{float}
\titleformat*{\section}{\large\bfseries}

\usepackage{tikz}
\usetikzlibrary{arrows}

\begin{document}


\section*{PSYC 402: Structural Equation Modelling}

By Jack Zhou

\begin{multicols*}{2}

% Any model can be thought of as a set of variables, of various sorts, to a set of parameter matrices of various sorts, 

% and a set of equations, that express the relationships between the variables.

% models are about explanation

\section{Algebra Review}

Let capital letters be matrices, lowercase letters be random vectors, and greek lowercase letters be constant vectors. We proceed with some definitions.

\subsection{Matrix Algebra}

TBD

\subsection{Expectation Algebra}

An expectation operator is denoted as $\mathbb{E}[\cdot]$, it has the following properties:

\begin{itemize}
    \item $\mathbb{E}[Az] = A \mathbb{E}[z]$
    \item $\mathbb{E}[\alpha] = \alpha$
    \item $\mathbb{E}[z + r] = \mathbb{E}[z] + \mathbb{E}[r]$
\end{itemize}

\subsection{Variance/Covariance Algebra}

The covariance operator is denoted as $\textrm{Cov}[\cdot]$. It is defined as follows:

\[\begin{aligned}
    \textrm{Cov}[x, y] &= \mathbb{E}\left[(x - \mathbb{E}[x])(y - \mathbb{E}[y])^T\right] \\
    & = \mathbb{E}\left[xy^T\right] - \mathbb{E}[x]\mathbb{E}[y]^T
\end{aligned}\]

If only one argument is given to $\textrm{Cov}[\cdot]$, then it is understood that both arguments are the same (ex. $\textrm{Cov}[a] = \textrm{Cov}[a, a] = \textrm{Var}[a]$).

Below are some properties of the covariance operator:

\begin{itemize}
    \item $\textrm{Cov}[Az] = A \textrm{Cov}[z] A^T$
    \item $\textrm{Cov}[x+y] = \textrm{Cov}[x] + \textrm{Cov}[x, y] + \textrm{Cov}[y, x] + \textrm{Cov}[y]$
    \item $\textrm{Cov}[x,y+z] = \textrm{Cov}[x,y] + \textrm{Cov}[x, z]$
    \item $\textrm{Cov}[x,\alpha] = 0$
\end{itemize}

The following result is particularly useful:

\[\begin{aligned}
    &\textrm{Cov}[Az+\dagger]\\
    =& \textrm{Cov}[Az] + \textrm{Cov}[Az,\dagger] + \textrm{Cov}[\dagger, Az] + \textrm{Cov}[\dagger]\\
    =& A\textrm{Cov}[z]A^T + A\textrm{Cov}[z,\dagger] + \textrm{Cov}[\dagger, z]A^T + \textrm{Cov}[\dagger]\\
\end{aligned}\]

$\dagger$ can either be a random vector or constant. If it is a constant, then $A\textrm{Cov}[z,\dagger] + \textrm{Cov}[\dagger, z]A^T + \textrm{Cov}[\dagger] = 0$.

\section{Three classes of models}

Structural Equations can be classified by 3 different classes.

\subsection{Path Model}

A path models can also be referred to as a multivariate regression model for observed cariables, or a causal model.

A path model can contain $p$ dependent (\textit{endogenous}) variables and $q$ independent (\textit{exogenous}) variables, denoted as the following:

\begin{itemize}
    \item $\mathbf{\vec{y}}$: a $p \times 1$ vector of \textit{endogenous} observed variables
    \item $\mathbf{\vec{x}}$: a $q \times 1$ vector of \textit{exogenous} observed variables
    \item $\mathbf{\vec{\zeta}}$: a $p \times 1$ vector of residuals
\end{itemize}

As suggested by the match of dimensionality between $\mathbf{\vec{y}}$ and $\mathbf{\vec{\zeta}}$, each of the entries in $\mathbf{\vec{y}}$ corresponds to a value in $\mathbf{\vec{\zeta}}$ with the same index. That is, for each dependent variable $\mathbf{\vec{y}}_i$, we have a corresponding residual $\mathbf{\vec{\zeta}}_i$ that the model is unable to account for.

It is important to note that path models do not contain any latent (unobserved) variables, as we can see in the following equation: 

\[\mathbf{\vec{y}} = [\mathbf{B} \mathbf{\vec{y}} + \mathbf{\Gamma} \mathbf{\vec{x}}] + \mathbf{\vec{\zeta}}\]

The general interpretation is that with this model, the set of endogenous variables are to be described in terms of a sum between the structural model $[\mathbf{B} \mathbf{\vec{y}} + \mathbf{\Gamma} \mathbf{\vec{x}}]$ and residuals $\mathbf{\vec{\zeta}}$.

The model consists of two parts, the $\mathbf{B} \mathbf{\vec{y}}$ term describes the regression of endogenous variables onto themselves, and the $\mathbf{\Gamma} \mathbf{\vec{x}}$ term describes the regression of exogenous variables onto the endogenous variables, in combined effort to best account for the endogenous variables.

One glaring issue is that in order to solve for $\mathbf{\vec{y}}$, it should only be on one side of the equation, not both. We can address this with algebra:

\[
\begin{aligned}
    \mathbf{\vec{y}} &= [\mathbf{B} \mathbf{\vec{y}} + \mathbf{\Gamma} \mathbf{\vec{x}}] + \mathbf{\vec{\zeta}} \\
    \mathbf{\vec{y}} - \mathbf{B} \mathbf{\vec{y}} &= \mathbf{\Gamma} \mathbf{\vec{x}} + \mathbf{\vec{\zeta}} \\
    (I - \mathbf{B}) \mathbf{\vec{y}} &= \mathbf{\Gamma} \mathbf{\vec{x}} + \mathbf{\vec{\zeta}} \\
    \mathbf{\vec{y}} &= (I - \mathbf{B})^{-1}(\mathbf{\Gamma} \mathbf{\vec{x}} + \mathbf{\vec{\zeta}}) \\
\end{aligned}    
\]

The equation $\mathbf{\vec{y}} = (I - \mathbf{B})^{-1}(\mathbf{\Gamma} \mathbf{\vec{x}} + \mathbf{\vec{\zeta}})$ is known as the reduced form, which is easier to work with mathematically.

Below are descriptions to all the matrices in a path model:
\begin{itemize}
    \item $\mathbf{B}$ : $p \times p$ regression coefficients for $\mathbf{\vec{y}}$
    \item $\mathbf{\Gamma}$ : $p \times q$ regression coefficients for $\mathbf{\vec{x}}$
    \item $\mathbf{\Phi}$ : $q \times q$ covariance matrix of $\mathbf{\vec{x}}$
    \item $\mathbf{\Psi}$ : $p \times p$ covariance matrix of $\mathbf{\vec{\zeta}}$
\end{itemize}

Note that $\mathbf{\Phi}$ and $\mathbf{\Psi}$ are matrices that aren't seen in the model. 

% TODO: move this to linalge review
For any covariance matrix $\mathbf{M}$, any $\mathbf{M}_{ii}$ entry is the variance of the $i$th variable and any $\mathbf{M}_{ij}$ entry is the covariance between the $i$th and $j$th variable.

$\mathbf{\Phi}$ can be considered a parameter for $\mathbf{\vec{y}}$. Therefore, we consider it as a starting point.

Often times we see correlations between residuals (ex. in longitudinal models), so $\mathbf{\Psi}$ does not necessarily have to be diagonal.

\subsubsection{Kinds of Path Models}

\begin{itemize}
    \item \textbf{Standard Multiple Regression}

    Suppose we have an endogenous variable $y_1$, its corresponding $\zeta_1$, and a set of exogenous variables $\{x_1,...,x_q\}$. A Standard Multiple Regression can be seen as a digraph with the following properties:
    
    \begin{itemize}
        \item An edge $(\zeta_1, y_1)$ exists; the residual impacts the endogenous variable
        \item For all $x_i$ in the set of exogenous variables, an edge $(x_i, y_1)$ exists; each of the exogenous varibales impacts the endogenous variable
        \item For all $x_i \neq x_j$, an edge $(x_i, x_j)$ exists to indicate correlations between exogenous variables
        
    \end{itemize}
    
    Let $\mathbf{\vec{x}}$ be a $p \times 1$ vector of exogenous variables, $\Gamma$ be a $1 \times q$ vector of regression coefficients for $\mathbf{\vec{x}}$, then such graph can be written as an equation as follows: \[y_1 = \Gamma \mathbf{\vec{x}} + \zeta_1\]

    \item \textbf{Standard Multivariate Regression}
    
    In this case, there are greater than $p > 1$ endogenous variables as well as $q > 1$ exogenous variables. 
    
    The setting is similar to the Standard Multiple Regression case, but there is more than one $y_i$, but no edges between any $y_i \neq y_j$ (no correlation between any endogenous variables).

    In addition to the notation used in standard multiple regression, we let $\mathbf{\vec{y}}$ be the $p \times 1$ vector of endogenous variables, and $\mathbf{B}$ be a $p \times p$ matrix as defined before. As the model asserts that there are no correlations between any endogenous variables, $\mathbf{B}$ is a zero matrix, and the covariance matrix of $\mathbf{\vec{y}}$, $\mathbf{\Psi}$, is taken to be diagonal.

    \item \textbf{First Order Autoregressive Time Series}
    
    A chain of measurements are collected in sequential order, imposing a temporal structure.

    The first measurement is considered exogenous, so we would name it $x_1$. From there on, each measurement that comes after are all endogenous variables $y_i$ with residuals $\zeta_i$. 
    
    Each variable is linearly dependent on the variable before, causing a chain of dependencies.
\end{itemize}

\subsection{Linear Factor/Measurement Models}

In the basic treatment, we have the following variables.

\begin{itemize}
    \item $\mathbf{\vec{x}}$: a $q \times 1$ vector of \textit{indicators} (observed variables)
    \item $\mathbf{\vec{\xi}}$: a $n \times 1$ vector of \textit{latent}, common-factor variables
    \item $\mathbf{\vec{\delta}}$: a $q \times 1$ vector of measurement error variables
\end{itemize}

In a parallel alternative form, we have these instead:

\begin{itemize}
    \item $\mathbf{\vec{y}}$: a $q \times 1$ vector of \textit{indicators} (observed variables)
    \item $\mathbf{\vec{\eta}}$: a $n \times 1$ vector of \textit{latent}, common-factor variables
    \item $\mathbf{\vec{\epsilon}}$: a $q \times 1$ vector of measurement error variables
\end{itemize}

Note that here we have a set of observed variables and a set of latent variables. The observed variables play the role in trying to determine the latent variables. Imagine measuring intelligence (abstract and latent) with test scores (indicators; concrete and observable).

Under Spearman, all $\mathbf{\vec{x}}_i$ is causally (regression) dependent upon a $\mathbf{\vec{\xi}}_i$ for some $i$. Moreover, upon regressing $\mathbf{\vec{x}}$ on $\mathbf{\vec{\xi}}$, within-correlations in $\mathbf{\vec{x}}$ no longer exist.

In other words, the reason why different $\mathbf{\vec{x}}_i$ could be correlated is because they arise from a common latent variable; $\mathbf{\vec{\xi}}$ can be taken as an explanation as to why within-correlations in $\mathbf{\vec{x}}$ exist.

We can summarize this model with the following formula:

\[\mathbf{\vec{x}} = \mathbf{\Lambda_x}\mathbf{\vec{\xi}} + \mathbf{\vec{\delta}}\]

Where $\mathbf{\Lambda_x}$ is $q \times n$ matrix called factor loadings, linking the indicators to the latent variables.

There is also a $n \times n$ matrix $\mathbf{\Phi}$, which is the covariance matrix of $\mathbf{\vec{\xi}}$. Typically there are many restriction on this matrix, for example we would assert it to be a diagonal matrix for orthogonality.

Finally, we also have covariance for $\mathbf{\vec{\delta}}$ as $\mathbf{\Theta_\delta}$, a $q \times q$ matrix. Recall that upon regression, the indicators become uncorrelated, suggesting $\mathbf{\Theta_\delta}$ is typically diagonal.

\subsubsection{Correlation accross sets of indicator/latent variables}

Sometimes we are interested in the correlation between multiple latent variables, and the only access we have towards them are their indicators, so the correlation would have to run on some linear weighted composite of the indicators per grouping.

The resulting correlation, which is used to indicate the correlation between the actual latent variables, is known to be less than the actual correlation between the latent variables.

To account for this issue, a mechanism of disattenuation is used. For two latent variables $\tau_1,\tau_2$, their correlation $\rho_{\tau_1,\tau_2}$ can be determined by a function of observed composites $c_1, c_2$ (respectively) as follows:

\[\rho_{\tau_1,\tau_2} = \frac{\rho_{c_1, c_2}}{\sqrt{\rho_{c_1, c_1'} \cdot \rho_{c_2, c_2'}}}\]

Where $\rho_{c_1, c_1'}$ denotes reliability for $c_1$, and likewise for $c_2$. This is known as the disattenuating correlation formula.

With structural equation modelling, we can add an edge to the path diagram that joins the latent variables and work from there, without the need for disattenuation.

\subsection{Multivariate Regression Model for Latent Variables}

Often known as the full model, a multivariate regression model for latent works like a causal model in latent terms. We have the following variables: 

\begin{itemize}
    \item $\mathbf{\vec{\eta}}$: a $m \times 1$ vector of latent endogenous variables
    \item $\mathbf{\vec{\xi}}$: a $n \times 1$ vector of latent exogenous variables
    \item $\mathbf{\vec{\zeta}}$: a $m \times 1$ vector of residuals
    \item $\mathbf{\vec{x}}$: a $q \times 1$ vector of indicators of $\mathbf{\vec{\xi}}$
    \item $\mathbf{\vec{y}}$: a $p \times 1$ vector of indicators of $\mathbf{\vec{\eta}}$
    \item $\mathbf{\vec{\delta}}$: a $q \times 1$ vector of measurement errors on $\mathbf{\vec{x}}$
    \item $\mathbf{\vec{\epsilon}}$: a $p \times 1$ vector of measurement errors on $\mathbf{\vec{y}}$
\end{itemize}

In terms of equations, we have the following:

\begin{itemize}
    \item \textbf{Regresion Model For Unobservables}
    \[\mathbf{\vec{\eta}} = \mathbf{B}\mathbf{\vec{\eta}} + \mathbf{\Gamma}\mathbf{\vec{\xi}} + \mathbf{\vec{\zeta}}\]
    \item \textbf{Measurement Model for $\mathbf{\vec{x}}$}
    \[\mathbf{\vec{x}} = \mathbf{\Lambda_x}\mathbf{\vec{\xi}} + \mathbf{\vec{\delta}}\]
    \item \textbf{Measurement Model for $\mathbf{\vec{y}}$}
    \[\mathbf{\vec{y}} = \mathbf{\Lambda_y}\mathbf{\vec{\eta}} + \mathbf{\vec{\epsilon}}\]
\end{itemize}

In line with the notation as previously introduced before, we have the following parameter matrices:

\begin{itemize}
    \item $\mathbf{B}$: a $m \times m$ regression matrix for $\mathbf{\vec{\eta}}$ on $\mathbf{\vec{\eta}}$
    
    As it makes no sense to say that an $\mathbf{\vec{\eta}}_i$ is dependent on $\mathbf{\vec{\eta}}_i$ itself, the diagonal elements of $\mathbf{B}$ are zero.

    \item $\mathbf{\Gamma}$: a $m \times n$ regression matrix for $\mathbf{\vec{\xi}}$ on $\mathbf{\vec{\eta}}$
    
    The entries in $\mathbf{\Gamma}$ are usually handpicked to be zero or nonzero. It essentially spells out our beliefs of how $\mathbf{\vec{\eta}}$ relates to $\mathbf{\vec{\xi}}$.

    \item $\mathbf{\Phi}$: a $n \times n$ covariance matrix of $\mathbf{\vec{\xi}}$
    \item $\mathbf{\Psi}$: a $m \times m$ covariance matrix of $\mathbf{\vec{\zeta}}$

    \item $\mathbf{\Lambda_x}$: a $ q \times n$ matrix of loadings

    \item $\mathbf{\Theta_\delta}$: a $q \times q$ covariance matrix of $\mathbf{\vec{x}}$

    \item $\mathbf{\Lambda_y}$: a $p \times m$ matrix of loadings

    \item $\mathbf{\Theta_\epsilon}$: a $p \times p$ covariance matrix of $\mathbf{\vec{y}}$
    
\end{itemize}

\section{Aims of Modelling}

A model is a candidate explanation of the joint distribution of a set of observable varibles. We place restrictions on parameter matrices in a model, reducing its complexity and also targetting specific aspects of the joint distribution. Typically targetting a set of association parameters within the distribution. In structural equation modelling, we are interested in the covariances $\mathbf{\Sigma}$ between the variables at the population level.

% ?
Placing restrictions effectively reduces the amount of parameters to estimate. Typically we want the number of parameters to estimate $t$ to be lower than the amount of input data elements $r$, so there needs to be lots of restrictions placed on the model.

By using a model with parameter matrices, we can denote the set of parameters to estimate as $\mathbf{\vec{\Theta}}$. We use $\mathbf{\vec{\Theta}}$ to estimate $\mathbf{\Sigma}$, denoted as $\mathbf{\hat{\Sigma}}(\mathbf{\vec{\Theta}})$, a function of $\mathbf{\vec{\Theta}}$. 

Once $\mathbf{\vec{\Theta}}$ is determined, we can find $\mathbf{\hat{\Sigma}}(\mathbf{\vec{\Theta}})$ and determine its likeliness to a sample estimate $s$ of $\mathbf{\vec{\Sigma}}$.

\section{Approach}

To begin, for a model from any of the three classes previously discussed, we proceed in a sequence of steps. We will discuss solely in terms of a observed variable path model, where no latent variables are considered.

\subsection{Rendering}

The rendering stage effectively generates a path diagram after specifying the observed and latent variables. The relationships between the variables are discussed and established and a diagram is drawn. 

The convention for drawing diagrams suggests drawing ellipsis for latent variables and rectangles for observed variables. Residuals are usually drawn with no enclosing shape.

% It is important to note that observed variables are either indicators or otherwise.

A single headed arrow denotes a regression dependency, where a double headed arrow denotes a correlational relationship.

Suppose a variable $a$ has a regression dependency on another variable $b$, and $b$ has a regression dependency on $a$, then a feedback loop is created and the entire model is said to be non-recursive.

\subsection{Mathematization}

The mathematization stage takes in a path diagram and re-expresses the diagram in terms of mathematical formulas.

Conventionally we have the following values to denote the number of variables in a model:

\begin{itemize}
    \item NX: Number of $\mathbf{x}$ variables
    \item NY: Number of $\mathbf{y}$ variables
    \item NK: Number of $\mathbf{\xi}$ variables
    \item NE: Number of $\mathbf{\eta}$ variables
    \item NZ: Number of $\mathbf{\zeta}$ variables
\end{itemize}

Note that since we are working with observed value path models, NK=NE=0.

Given these values, we can determine the dimensionality of $\mathbf{B}$, $\mathbf{\Gamma}$, $\mathbf{\Phi}$, $\mathbf{\Psi}$ and fill in entries based on connections in the path diagram. 

Often the parameters in the path diagram have subscripts that determine which index it belongs in for the corresponding matrix. For example, $\gamma_{13}$ indicates that it belongs to row 1 column 3 for $\mathbf{\Gamma}$.

Usually for $\mathbf{\Phi}$ and $\mathbf{\Psi}$ the parameters may not be indicated explicitly, but as a general rule the diagonal elements (the variances) are by default nonzero.

From this information, we can generate a model implied covariance matrix $\hat{\mathbf{\Sigma}}(\mathbf{\vec{\Theta}})$, note that it is a matrix for the joint distribution, which means its dimensionality is $(NX+NY) \times (NX+NY)$. We can partition $\hat{\mathbf{\Sigma}}(\mathbf{\vec{\Theta}})$ as follows:

\[
\hat{\mathbf{\Sigma}}(\mathbf{\vec{\Theta}}) = 
\left[
    \begin{array}{c|c}
        \hat{\mathbf{\Sigma}}_y(\mathbf{\vec{\Theta}}) & 
        \hat{\mathbf{\Sigma}}_{yx}(\mathbf{\vec{\Theta}})\\
        \hline
        \hat{\mathbf{\Sigma}}_{xy}(\mathbf{\vec{\Theta}}) &
        \mathbf{\Phi}
    \end{array}
\right]
\]

Where 
\[
\hat{\mathbf{\Sigma}}_y(\mathbf{\vec{\Theta}})
= (I - \mathbf{B})^{-1}
\left[
\mathbf{\Gamma}\mathbf{\Phi}\mathbf{\Gamma}^T 
+ \mathbf{\Psi}
\right]
((I - \mathbf{B})^{-1})^T
\]

and 

\[
\hat{\mathbf{\Sigma}}_{yx}(\mathbf{\vec{\Theta}})
= (I - \mathbf{B})^{-1}\mathbf{\Gamma}^T\mathbf{\Phi}
\]

\subsection{Identification}

TBD

\end{multicols*}
\end{document}